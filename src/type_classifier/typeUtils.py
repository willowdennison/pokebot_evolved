import pandas as pd
from nltk.tokenize.sonority_sequencing import SyllableTokenizer
from nltk.tokenize import word_tokenize

#all characters legally generated by base model
legal_chars = ['\n', ' ', '-', '.', '2', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

#functions to encode and decode list[char]
char_to_int = dict((c, i) for i, c in enumerate(legal_chars))
int_to_char = dict((i, c) for c, i in char_to_int.items())

tokenizer = SyllableTokenizer()

def tokenizeText(text):
    # take input string, return list[int] of each character from the string tokenized
    try: 
        text = text.lower()
    except AttributeError:
        return [0] #handle NaN
    chars = [char_to_int[char] for char in list(text) if char in legal_chars]
    return chars

def tokenizeNameSyllables(name):
    # take input string, return list[list[int]] of the initial string broken into syllables, and each syllable encoded with tokenizeText()
    syllabized = syllabize(name)
    for i in range(len(syllabized)):
        syllabized[i] = [char_to_int[char] for char in syllabized[i]]
    print(syllabized)
    return syllabized

def syllabize(name):
    
    name = word_tokenize(name)
            
    wordsSyllabized = []
    for word in name: #handle multi word pokemon names
        wordsSyllabized.append(tokenizer.tokenize(word))
            
    syllables = []
    for word in wordsSyllabized: #combine multi word syllable lists
        for syllable in word:
            if syllable not in ['-', '.', '(', ')']:
                syllables.append(syllable.lower())
            
    syllables = tokenizer.validate_syllables(syllables)
    
    return syllables

def encodeType(type_str):
    # takes str containing a valid type / nan (case-insensitive) and returns the int representation (between 0 and 18)
    
    try: 
        type_str = type_str.lower()
    except AttributeError:
        return 18 #handle NaN
    type_int = None
    
    match type_str:
        case 'fire': type_int = 0
        case 'water': type_int = 1
        case 'grass': type_int = 2
        case 'electric': type_int = 3
        case 'ice': type_int = 4
        case 'fighting': type_int = 5
        case 'poison': type_int = 6
        case 'ground': type_int = 7
        case 'flying': type_int = 8
        case 'psychic': type_int = 9
        case 'bug': type_int = 10
        case 'rock': type_int = 11
        case 'ghost': type_int = 12
        case 'dark': type_int = 13
        case 'dragon': type_int = 14
        case 'steel': type_int = 15
        case 'fairy': type_int = 16
        case 'normal': type_int = 17
        case 'nan': type_int=18
        case _: raise ValueError('Invalid type entered')
        
    return type_int

def decodeType(type_int):
    # takes int representing a valid type / nan (between 0 and 18) and returns the str equivelant
    type_str = None
    match type_int:
        case 0: type_str = 'fire'
        case 1: type_str = 'water'
        case 2: type_str = 'grass'
        case 3: type_str = 'electric'
        case 4: type_str = 'ice'
        case 5: type_str = 'fighting'
        case 6: type_str = 'poison'
        case 7: type_str = 'ground'
        case 8: type_str = 'flying'
        case 9: type_str = 'psychic'
        case 10: type_str = 'bug'
        case 11: type_str = 'rock'
        case 12: type_str = 'ghost'
        case 13: type_str = 'dark'
        case 14: type_str = 'dragon'
        case 15: type_str = 'steel'
        case 16: type_str = 'fairy'
        case 17: type_str = 'normal'
        case 18: type_str = 'nan'
        case _: raise ValueError('Invalid type entered')
        
    return type_str

def encodeName(name):
    
    name = tokenizeText(name)
    
    while len(name) < 12:
        name.append(int(0)) 
    
    return name